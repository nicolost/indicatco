{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit stock market prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull data\n",
    "djia_data = pd.read_csv('./stocknews/Combined_News_DJIA.csv', parse_dates=True)\n",
    "\n",
    "djia_data['combined'] = djia_data[djia_data.columns[2:]].apply(\n",
    "    lambda x: ' '.join(x.astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "def sanitise_row(row):\n",
    "    return re.sub('[^A-Za-z ]+', '', row.replace(\"b\\\"\", \"\").replace(\"b'\", \"\"))\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    sentence = sanitise_row(sentence)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "djia_data['combined'] = djia_data['combined'].map(lambda x: remove_stop_words(x))\n",
    "\n",
    "# print(type(djia_data['combined']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5354449472096531\n"
     ]
    }
   ],
   "source": [
    "# Ratio of label 1s and 0s\n",
    "data_label = np.array(djia_data['Label'])\n",
    "\n",
    "print(np.sum(data_label == 1) / data_label.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare train and test data\n",
    "all_data = djia_data['combined']\n",
    "data_label = np.array(djia_data['Label'])\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(all_data)\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_data, data_label, test_size=0.50, random_state=23)\n",
    "# we also need dictionary that maps word to number of occurences -> \n",
    "# to handle cases where word is not found in the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert training data into a vector for LSTM\n",
    "# how does the vector look like?\n",
    "# need to build word_index first!\n",
    "\n",
    "def gen_word_indexes(data): \n",
    "    word_index = {}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2  # unknown\n",
    "    word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "    index = 4\n",
    "    # loop through everything in all_data\n",
    "    for row in data:\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            if word not in word_index:\n",
    "                word_index[word] = index\n",
    "                index += 1\n",
    "\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    return (word_index, reverse_word_index)\n",
    "\n",
    "\n",
    "def vectorise_text_data(data, word_index):\n",
    "    vectorised_data = []\n",
    "    for row in data:\n",
    "        current_row = []\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            current_row.append(word_index[word])\n",
    "        vectorised_data.append(current_row)\n",
    "    return vectorised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(word_index, reverse_word_index) = gen_word_indexes(all_data)\n",
    "X_vectorised = vectorise_text_data(all_data, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193, 80) (796, 80)\n",
      "[ 2512 16506 12074 16654 16655 16656 16657  2253   195 12074  1047  4551\n",
      "  4713  6332 16658  1047  4996 10159    27    71  2215  4613  1047  8849\n",
      "  1344   285  6204 11106  6792 16161  1193  2496 16644 16369 16659  1616\n",
      "   571  2012 16660  4875   712   713  6624   802  4916   617  1406  3799\n",
      " 16661   723   310 16662  3071  2206  3389  6107 13766   672  5422 16663\n",
      " 16664  2419  7464  2413  1101   600 11961    89 12168    83   520   156\n",
      "  2079  1057 16665    19  1340  1117   932 16666]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# number of most-frequent words to use\n",
    "nb_words = 40000\n",
    "# cut texts after this number of words\n",
    "maxlen = 80\n",
    "\n",
    "X_v_train, X_v_test, y_v_train, y_v_test = train_test_split(\n",
    "    X_vectorised, data_label, test_size=0.40, random_state=23)\n",
    "\n",
    "X_v_pad_train = sequence.pad_sequences(X_v_train,\n",
    "                                       value=word_index[\"<PAD>\"],\n",
    "                                       padding='post',\n",
    "                                       maxlen=maxlen)\n",
    "X_v_pad_test = sequence.pad_sequences(X_v_test, \n",
    "                                      value=word_index[\"<PAD>\"],\n",
    "                                      padding='post',\n",
    "                                      maxlen=maxlen)\n",
    "\n",
    "print(X_v_pad_train.shape, X_v_pad_test.shape)\n",
    "print(X_v_pad_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? We train!\n",
    "\n",
    "### Training using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammad/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5386934673366834"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LogisticRegression(random_state=0, solver='lbfgs')\n",
    "\n",
    "v = CountVectorizer()\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', v),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "EMBEDDING_DIM = 256 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 64 # total LSTM units\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=EMBEDDING_DIM, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(LSTM_DIM, dropout=0.75, recurrent_dropout=0.7))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 715 samples, validate on 478 samples\n",
      "Epoch 1/10\n",
      "715/715 [==============================] - 4s 5ms/step - loss: 0.6830 - acc: 0.6042 - val_loss: 0.6888 - val_acc: 0.5962\n",
      "Epoch 2/10\n",
      "715/715 [==============================] - 3s 5ms/step - loss: 0.6810 - acc: 0.6308 - val_loss: 0.6879 - val_acc: 0.5879\n",
      "Epoch 3/10\n",
      "715/715 [==============================] - 3s 5ms/step - loss: 0.6800 - acc: 0.6098 - val_loss: 0.6869 - val_acc: 0.5858\n",
      "Epoch 4/10\n",
      "715/715 [==============================] - 3s 5ms/step - loss: 0.6682 - acc: 0.6545 - val_loss: 0.6858 - val_acc: 0.5858\n",
      "Epoch 5/10\n",
      "715/715 [==============================] - 3s 5ms/step - loss: 0.6657 - acc: 0.6769 - val_loss: 0.6846 - val_acc: 0.5628\n",
      "Epoch 6/10\n",
      "715/715 [==============================] - 4s 5ms/step - loss: 0.6605 - acc: 0.6615 - val_loss: 0.6836 - val_acc: 0.5586\n",
      "Epoch 7/10\n",
      "715/715 [==============================] - 3s 5ms/step - loss: 0.6439 - acc: 0.6853 - val_loss: 0.6826 - val_acc: 0.5628\n",
      "Epoch 8/10\n",
      "715/715 [==============================] - 3s 5ms/step - loss: 0.6296 - acc: 0.7636 - val_loss: 0.6817 - val_acc: 0.5669\n",
      "Epoch 9/10\n",
      "715/715 [==============================] - 3s 5ms/step - loss: 0.6241 - acc: 0.7147 - val_loss: 0.6811 - val_acc: 0.5711\n",
      "Epoch 10/10\n",
      "715/715 [==============================] - 3s 5ms/step - loss: 0.5959 - acc: 0.7664 - val_loss: 0.6809 - val_acc: 0.5753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17021b4e0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "model.fit(X_v_pad_train, y_v_train, epochs=10, batch_size=batch_size, \n",
    "          shuffle=True, validation_split=0.4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 55.28%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate( X_v_pad_test, y_v_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
