{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit stock market prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull data\n",
    "djia_data = pd.read_csv('./stocknews/Combined_News_DJIA.csv', parse_dates=True)\n",
    "\n",
    "djia_data['combined'] = djia_data[djia_data.columns[2:]].apply(\n",
    "    lambda x: ' '.join(x.astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "def sanitise_row(row):\n",
    "    return re.sub('[^A-Za-z ]+', '', row.replace(\"b\\\"\", \"\").replace(\"b'\", \"\"))\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    sentence = sanitise_row(sentence)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "djia_data['combined'] = djia_data['combined'].map(lambda x: remove_stop_words(x))\n",
    "\n",
    "# print(type(djia_data['combined']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5354449472096531\n"
     ]
    }
   ],
   "source": [
    "# Ratio of label 1s and 0s\n",
    "data_label = np.array(djia_data['Label'])\n",
    "\n",
    "print(np.sum(data_label == 1) / data_label.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare train and test data\n",
    "all_data = djia_data['combined']\n",
    "data_label = np.array(djia_data['Label'])\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(all_data)\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_data, data_label, test_size=0.50, random_state=23)\n",
    "# we also need dictionary that maps word to number of occurences -> \n",
    "# to handle cases where word is not found in the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert training data into a vector for LSTM\n",
    "# how does the vector look like?\n",
    "# need to build word_index first!\n",
    "\n",
    "def gen_word_indexes(data): \n",
    "    word_index = {}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2  # unknown\n",
    "    word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "    index = 4\n",
    "    # loop through everything in all_data\n",
    "    for row in data:\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            if word not in word_index:\n",
    "                word_index[word] = index\n",
    "                index += 1\n",
    "\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    return (word_index, reverse_word_index)\n",
    "\n",
    "\n",
    "def vectorise_text_data(data, word_index):\n",
    "    vectorised_data = []\n",
    "    for row in data:\n",
    "        current_row = []\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            if word not in word_index:\n",
    "                current_row.append(word_index[\"<UNUSED>\"])\n",
    "            else:\n",
    "                current_row.append(word_index[word])\n",
    "        vectorised_data.append(current_row)\n",
    "    return vectorised_data\n",
    "\n",
    "# really slow :(\n",
    "def aggregate_previous_days(num_days, data):\n",
    "    for i in range(num_days, len(data) - num_days):\n",
    "        for d in range(i, i + num_days):\n",
    "            data[i] += ' ' + data[d]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(word_index, reverse_word_index) = gen_word_indexes(all_data)\n",
    "# save word_index somewhere\n",
    "import pickle\n",
    "pickle_out = open(\"word_index.pickle\",\"wb\")\n",
    "pickle.dump(word_index, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "X_vectorised = vectorise_text_data(all_data, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(994, 600) (995, 600)\n",
      "[  444  2131  6102  3963  5816  1949  1096 10617  1638  2131  3239  2961\n",
      "  3963  5816  1075   225   392  1949 10306  1802 25540  6158   100  2814\n",
      "  7770 28459  4976   306   307  1623   340  2313  1646  2379  2218  1808\n",
      "    89 25884 28460  1623   180 12995 12996  2982   875  5419  5420   953\n",
      "  4494  9126 10574  3310  5419  5420  1219  7572  4494  2103  9126  1258\n",
      "   692 11386 23245  1049  1802  3925   505 22284  7422  2015  1507  2149\n",
      "  8131  4398  1518 20111   505   365  2971  8084  7901  1469  8583   284\n",
      "  5073  8084  3488   125   625 12480  1029   231  2711   330  1623    93\n",
      "  1095  8553   546   142  2922   365  2806  1522 28461 28462  4761  1052\n",
      " 28463  2433  1098  3182 28464  1096 14717 25209 15720    89   184  1623\n",
      "  2157    11  1179    12  1955    26 27080  1222  1761  5100   580  1829\n",
      "  1142   758  1044  1519    58  1002   310  2295   374    54  1142   758\n",
      "  7566 10179   712  8397  1870  3760  2207   431  2220   310  8749  1044\n",
      "   444     6    66   634    58  2935  1832  1309  1707 10426  4657   321\n",
      "  4081  2369  4494  1857  1005  1142  1001    58  1224  7290 11977  1356\n",
      "  2439  3686  1221   297  6221  2936  4589  2164  1221   297  6346  6221\n",
      "  1959  9936 15671   867  5811  1646  2379 10172  1623   279    89   861\n",
      "   862  1120 11685  3137  5012    12  1832 14019  5425  5919 28465 28466\n",
      "  1872  3501   735 18866   326 22706 11357 28467   169  1477  1439  3287\n",
      "   847  2579   385    93  8123   264  5482  1921  5227   257 18481  5635\n",
      "  2079 28468  7802 16295   593   181  1003  3121  1921  4070  5482   257\n",
      " 11817  5635  1429   177 12002  2001  3019    54  5127  2091    58 16376\n",
      "   631  2554  2093  6925  1926   471   405    54  1004   665   292  5257\n",
      "   305  1409  3913  1646    89  1215    58  5330   315   152  5472  1742\n",
      "   142 28469 28470 11608  1623  3572  1581   330 28471  1477  3036    93\n",
      "    26    19    10  2846  1623  1761  3065  2632    89  1623  7643 12222\n",
      "  1884    12   783  6625  5278  1498 28472  1045  4976  3117  6229   448\n",
      " 28473 28474 28475  2544  4543  3312  1641  4519  3836  9259  5833  1041\n",
      "  1389  9259 27998   828  7346   712 28476    40   304  7549    42   142\n",
      " 22685   870  3864   847  4951  6341  1017  1154   476  2841 12796   365\n",
      "   374 28477  5243 11405   739 10352  2779  9655   476 28478 14742  1289\n",
      "  3203   828   584  6471 16631  8646  3583    58  1375 15761   841  2615\n",
      "    58  6230  1371 23107    66   428  5596   352  4212  9717  1977 17746\n",
      "  2505   769   538  6229  3811  5393  5106   869  7475 20176     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# number of most-frequent words to use\n",
    "nb_words = 40000\n",
    "# cut texts after this number of words\n",
    "maxlen = 600\n",
    "\n",
    "X_v_train, X_v_test, y_v_train, y_v_test = train_test_split(\n",
    "    X_vectorised, data_label, test_size=0.50, random_state=23)\n",
    "\n",
    "X_v_pad_train = sequence.pad_sequences(X_v_train,\n",
    "                                       value=word_index[\"<PAD>\"],\n",
    "                                       padding='post',\n",
    "                                       maxlen=maxlen)\n",
    "X_v_pad_test = sequence.pad_sequences(X_v_test, \n",
    "                                      value=word_index[\"<PAD>\"],\n",
    "                                      padding='post',\n",
    "                                      maxlen=maxlen)\n",
    "\n",
    "print(X_v_pad_train.shape, X_v_pad_test.shape)\n",
    "print(X_v_pad_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? We train!\n",
    "\n",
    "### Training using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammad/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5386934673366834"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LogisticRegression(random_state=0, solver='lbfgs')\n",
    "\n",
    "v = CountVectorizer()\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', v),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 64 # total LSTM units\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=EMBEDDING_DIM, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(LSTM_DIM, dropout=0.8, recurrent_dropout=0.8))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 695 samples, validate on 299 samples\n",
      "Epoch 1/7\n",
      "695/695 [==============================] - 13s 18ms/step - loss: 0.6980 - acc: 0.4935 - val_loss: 0.6908 - val_acc: 0.5585\n",
      "Epoch 2/7\n",
      "695/695 [==============================] - 13s 18ms/step - loss: 0.6995 - acc: 0.4647 - val_loss: 0.6915 - val_acc: 0.5585\n",
      "Epoch 3/7\n",
      "695/695 [==============================] - 13s 18ms/step - loss: 0.6966 - acc: 0.5007 - val_loss: 0.6914 - val_acc: 0.5585\n",
      "Epoch 4/7\n",
      "695/695 [==============================] - 13s 19ms/step - loss: 0.6994 - acc: 0.4964 - val_loss: 0.6912 - val_acc: 0.5585\n",
      "Epoch 5/7\n",
      "695/695 [==============================] - 13s 18ms/step - loss: 0.6914 - acc: 0.5295 - val_loss: 0.6915 - val_acc: 0.5585\n",
      "Epoch 6/7\n",
      "695/695 [==============================] - 13s 18ms/step - loss: 0.6959 - acc: 0.4964 - val_loss: 0.6918 - val_acc: 0.5585\n",
      "Epoch 7/7\n",
      "695/695 [==============================] - 14s 21ms/step - loss: 0.6961 - acc: 0.4777 - val_loss: 0.6919 - val_acc: 0.5585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16433c9b0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit(X_v_pad_train, y_v_train, epochs=7, batch_size=batch_size, \n",
    "          shuffle=True, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 55.08%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_v_pad_test, y_v_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
