{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit stock market prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull data\n",
    "djia_data = pd.read_csv('./stocknews/Combined_News_DJIA.csv', parse_dates=True)\n",
    "\n",
    "djia_data['combined'] = djia_data[djia_data.columns[2:]].apply(\n",
    "    lambda x: ' '.join(x.astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "def sanitise_row(row):\n",
    "    return re.sub('[^A-Za-z ]+', '', row.replace(\"b\\\"\", \"\").replace(\"b'\", \"\"))\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    sentence = sanitise_row(sentence)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "djia_data['combined'] = djia_data['combined'].map(lambda x: remove_stop_words(x))\n",
    "\n",
    "# print(type(djia_data['combined']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5354449472096531\n"
     ]
    }
   ],
   "source": [
    "# Ratio of label 1s and 0s\n",
    "data_label = np.array(djia_data['Label'])\n",
    "\n",
    "print(np.sum(data_label == 1) / data_label.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare train and test data\n",
    "all_data = djia_data['combined']\n",
    "data_label = np.array(djia_data['Label'])\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(all_data)\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_data, data_label, test_size=0.80, random_state=23)\n",
    "# we also need dictionary that maps word to number of occurences -> \n",
    "# to handle cases where word is not found in the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert training data into a vector for LSTM\n",
    "# how does the vector look like?\n",
    "# need to build word_index first!\n",
    "\n",
    "\n",
    "# generate word numbers\n",
    "def gen_word_indexes(data): \n",
    "    word_index = {}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2  # unknown\n",
    "    word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "    index = 4\n",
    "    # loop through everything in all_data\n",
    "    for row in data:\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            if word not in word_index:\n",
    "                word_index[word] = index\n",
    "                index += 1\n",
    "\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    return (word_index, reverse_word_index)\n",
    "\n",
    "\n",
    "# text --> numbers corresponding to words\n",
    "def vectorise_text_data(data, word_index):\n",
    "    vectorised_data = []\n",
    "    for row in data:\n",
    "        current_row = []\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            if word not in word_index:\n",
    "                current_row.append(word_index[\"<UNUSED>\"])\n",
    "            else:\n",
    "                current_row.append(word_index[word])\n",
    "        vectorised_data.append(current_row)\n",
    "    return vectorised_data\n",
    "\n",
    "def aggregate_previous_days(num_days, data):\n",
    "    new = data.copy()\n",
    "    for i in range(num_days, len(data) - num_days):\n",
    "        for d in range(1, num_days):\n",
    "            new.at[i] = new[i] + \" \" + data[d+i-1]\n",
    "    return new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(word_index, reverse_word_index) = gen_word_indexes(all_data)\n",
    "# save word_index somewhere\n",
    "# import pickle\n",
    "# pickle_out = open(\"word_index.pickle\",\"wb\")\n",
    "# pickle.dump(word_index, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "X_vectorised = vectorise_text_data(all_data, word_index)\n",
    "# time_series_data = aggregate_previous_days(5, all_data)\n",
    "# X_series_vectorised = vectorise_text_data(time_series_data, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 1 1 1]\n",
      "sizes1984 1984\n"
     ]
    }
   ],
   "source": [
    "# print(data_label)\n",
    "# data_label_5day = data_label.copy()\n",
    "\n",
    "# # remove the first 4 and last data label:\n",
    "# data_label_5day = data_label_5day[5:]\n",
    "# X_5day_vectorised = X_series_vectorised[4:-1]\n",
    "\n",
    "# print(\"sizes\" + str(len(X_5day_vectorised)) + \" \" +  str(len(data_label_5day)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1591, 1000) (398, 1000)\n",
      "[   7 1003 1004 3171 3172 3173 3174 1530   16 3175 1812   83  758 1318\n",
      " 1252 3176 3177   89 3178 3179 3180   12  794 3181   81 3182 2841  180\n",
      "  586 3183 2128   19   35   83 1339 1761 2388  100 3114 3184  499 1098\n",
      "  942 3185 3186  527 3187 3188  599  847  128 1055 3189   42   83 2487\n",
      "  879  836 3190 1923   83 2469 3191   16 3192 3193 2384 3194 1996   87\n",
      " 3195 3196 1350 1129 1138 1364 1365 2540   87 3197 3198 3199   40 3131\n",
      " 2953 3132 1021 1778  163 3200 1746  715   83  163  797 1673 3201 2854\n",
      " 3202 3203 1768 3204 3205 3206  589 3207 3208 3209 3210 1749 1332  797\n",
      " 1695 3211 1212 3212 3213 3214 2739  215 3215 2576   58 3216 1872 3217\n",
      " 3218  156  973 1096 3219 3220 1388 2423 3221 1628  180 2518 2602 3222\n",
      " 3223 3224 1794 2883 3225  289  752  272 3226  121 1283 3227 2789  318\n",
      " 2536 1812 3228  197 3229  258 2485 3230 2067  280 3231 2519 1313    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# number of most-frequent words to use\n",
    "nb_words = 40000\n",
    "# cut texts after this number of words\n",
    "maxlen = 1000\n",
    "\n",
    "# we can change the \"X???_vectorised\" based on the number of cumulative days:\n",
    "X_v_train, X_v_test, y_v_train, y_v_test = train_test_split(\n",
    "    X_vectorised, data_label, test_size=0.20, random_state=32)\n",
    "\n",
    "X_v_pad_train = sequence.pad_sequences(X_v_train,\n",
    "                                       value=word_index[\"<PAD>\"],\n",
    "                                       padding='post',\n",
    "                                       maxlen=maxlen)\n",
    "X_v_pad_test = sequence.pad_sequences(X_v_test, \n",
    "                                      value=word_index[\"<PAD>\"],\n",
    "                                      padding='post',\n",
    "                                      maxlen=maxlen)\n",
    "\n",
    "print(X_v_pad_train.shape, X_v_pad_test.shape)\n",
    "print(X_v_pad_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? We train!\n",
    "\n",
    "### Training using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.535175879396985"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LogisticRegression(random_state=0, solver='lbfgs')\n",
    "\n",
    "v = CountVectorizer()\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', v),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 64 # total LSTM units\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=EMBEDDING_DIM, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(LSTM_DIM, dropout=0.8, recurrent_dropout=0.8))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tom/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1110 samples, validate on 477 samples\n",
      "Epoch 1/7\n",
      "1110/1110 [==============================] - 15s 14ms/step - loss: 0.6950 - acc: 0.5090 - val_loss: 0.6924 - val_acc: 0.5094\n",
      "Epoch 2/7\n",
      "1110/1110 [==============================] - 14s 12ms/step - loss: 0.6916 - acc: 0.5234 - val_loss: 0.6925 - val_acc: 0.5115\n",
      "Epoch 3/7\n",
      "1110/1110 [==============================] - 14s 13ms/step - loss: 0.6903 - acc: 0.5216 - val_loss: 0.6926 - val_acc: 0.5115\n",
      "Epoch 4/7\n",
      " 256/1110 [=====>........................] - ETA: 9s - loss: 0.6874 - acc: 0.5547 "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_v_pad_train, y_v_train, epochs=7, batch_size=batch_size, \n",
    "          shuffle=True, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 53.90%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_v_pad_test, y_v_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to the disk\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#serialize to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "#serialize weights to HDF5\n",
    "model.save_weights(\"model3.h5\")\n",
    "print(\"Saved model to the disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209],\n",
       "       [0.4938209]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load some stuff\n",
    "\n",
    "json_file = open('model2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# get data from json\n",
    "data_from_server = [\n",
    "    \"If You Cannot Trust Saudis With Bone Saw, Says US Lawmaker, 'You Should Not Trust Them With Nuclear Weapons': Trump administration's secret authorizations of nuclear technology sales to Saudi Arabia spark alarm in Congress\",\n",
    "    \"French healthcare system 'should not fund homeopathy' - French medical and drug experts say homeopathic medicines should no longer be paid for by the country\\u2019s health system because there is no evidence they work.\",\n",
    "    \"Trump says Navy SEAL charged with war crimes, committing premeditated murder and shooting at unarmed Iraqi civilians, will be moved to 'less restrictive confinement'\",\n",
    "    \"Years of Mark Zuckerberg's old Facebook posts have vanished. The company says it 'mistakenly deleted' them.\",\n",
    "    \"France and Germany hold historic first joint parliamentary session, commit to joint defence and \\\"a common military culture\\\"\",\n",
    "    \"Russia tells Trump its troops will stay in Venezuela for 'as long as needed' in a blunt rejection of his demand they leave immediately\",\n",
    "    \"Trump cuts all direct assistance to Northern Triangle countries Honduras, El Salvador, Guatemala\",\n",
    "    \"Churchill's policies caused millions of Indian famine deaths, study says\",\n",
    "    \"Man jailed for harbouring Paris attackers\",\n",
    "    \"Secret tape increases pressure on Trudeau\",\n",
    "    \"Puerto Rico governor warns White House: 'If the bully gets close, I'll punch the bully in the mouth'\",\n",
    "    \"Philippines beach resort slams 'freeloading' social media influencers\",\n",
    "    \"Huawei says US has 'loser's attitude' because it can't compete\",\n",
    "    \"A new study reveals the Amazon is losing surface water | A major new study of the Amazon has revealed an alarming trend, with the region losing as much as 350 km2 of surface freshwater every year on average. The loss is related to the construction of hydropower dams, deforestation and climate change\",\n",
    "    \"The day North Korea talks collapsed, Trump passed Kim a note demanding he turn over his nukes\",\n",
    "    \"Burundi bans BBC and suspends Voice of America, activists cry foul\",\n",
    "    \"Schoolboy finds lost medieval gravestone\",\n",
    "    \"Questions are mounting over special counsel Mueller\\u2019s inquiry into whether Trump obstructed justice as lawmakers on Capitol Hill await the release of his report.\",\n",
    "    \"China's Antarctic bases within Australia's claim are going unchecked\",\n",
    "    \"Norway agreed on Thursday to hand back thousands of artefacts removed from Easter Island by the explorer Thor Heyerdahl during his trans-Pacific raft expeditions in the 1950s.\",\n",
    "    \"Brunei defends tough new Islamic laws that would allow death by stoning for adultery and homosexuality against growing backlash\",\n",
    "    \"Pope, Morocco's king, say Jerusalem must be open to all faiths\",\n",
    "    \"Video of father and son illegally killing mama bear, shrieking cubs released in Alaska\",\n",
    "    \"WTO rules against the US and Boeing in mammoth trade row with the EU\",\n",
    "    \"French 'yellow vests' stage 20th day of protests\"\n",
    "]\n",
    "\n",
    "data_from_server_vector = vectorise_text_data(data_from_server, word_index)\n",
    "\n",
    "data_from_server_vector_pad = sequence.pad_sequences(data_from_server_vector, \n",
    "                                      value=word_index[\"<PAD>\"],\n",
    "                                      padding='post',\n",
    "                                      maxlen=maxlen)\n",
    "\n",
    "loaded_model.predict(data_from_server_vector_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
