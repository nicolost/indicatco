{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit stock market prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull data\n",
    "djia_data = pd.read_csv('./stocknews/Combined_News_DJIA.csv', parse_dates=True)\n",
    "\n",
    "djia_data['combined'] = djia_data[djia_data.columns[2:]].apply(\n",
    "    lambda x: ' '.join(x.astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "def sanitise_row(row):\n",
    "    return re.sub('[^A-Za-z ]+', '', row.replace(\"b\\\"\", \"\").replace(\"b'\", \"\"))\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    sentence = sanitise_row(sentence)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "djia_data['combined'] = djia_data['combined'].map(lambda x: remove_stop_words(x))\n",
    "\n",
    "# print(type(djia_data['combined']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5354449472096531\n"
     ]
    }
   ],
   "source": [
    "# Ratio of label 1s and 0s\n",
    "data_label = np.array(djia_data['Label'])\n",
    "\n",
    "print(np.sum(data_label == 1) / data_label.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare train and test data\n",
    "all_data = djia_data['combined']\n",
    "data_label = np.array(djia_data['Label'])\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(all_data)\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_data, data_label, test_size=0.80, random_state=23)\n",
    "# we also need dictionary that maps word to number of occurences -> \n",
    "# to handle cases where word is not found in the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert training data into a vector for LSTM\n",
    "# how does the vector look like?\n",
    "# need to build word_index first!\n",
    "\n",
    "\n",
    "# generate word numbers\n",
    "def gen_word_indexes(data): \n",
    "    word_index = {}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2  # unknown\n",
    "    word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "    index = 4\n",
    "    # loop through everything in all_data\n",
    "    for row in data:\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            if word not in word_index:\n",
    "                word_index[word] = index\n",
    "                index += 1\n",
    "\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    return (word_index, reverse_word_index)\n",
    "\n",
    "\n",
    "# text --> numbers corresponding to words\n",
    "def vectorise_text_data(data, word_index):\n",
    "    vectorised_data = []\n",
    "    for row in data:\n",
    "        current_row = []\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            if word not in word_index:\n",
    "                current_row.append(word_index[\"<UNUSED>\"])\n",
    "            else:\n",
    "                current_row.append(word_index[word])\n",
    "        vectorised_data.append(current_row)\n",
    "    return vectorised_data\n",
    "\n",
    "def aggregate_previous_days(num_days, data):\n",
    "    new = data.copy()\n",
    "    for i in range(num_days, len(data) - num_days):\n",
    "        for d in range(1, num_days):\n",
    "            new.at[i] = new[i] + \" \" + data[d+i-1]\n",
    "    return new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(word_index, reverse_word_index) = gen_word_indexes(all_data)\n",
    "# save word_index somewhere\n",
    "import pickle\n",
    "pickle_out = open(\"word_index.pickle\",\"wb\")\n",
    "pickle.dump(word_index, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# X_vectorised = vectorise_text_data(all_data, word_index)\n",
    "time_series_data = aggregate_previous_days(5, all_data)\n",
    "X_series_vectorised = vectorise_text_data(time_series_data, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 1 1 1]\n",
      "sizes1984 1984\n"
     ]
    }
   ],
   "source": [
    "print(data_label)\n",
    "data_label_5day = data_label.copy()\n",
    "\n",
    "# remove the first 4 and last data label:\n",
    "data_label_5day = data_label_5day[5:]\n",
    "X_5day_vectorised = X_series_vectorised[4:-1]\n",
    "\n",
    "print(\"sizes\" + str(len(X_5day_vectorised)) + \" \" +  str(len(data_label_5day)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1587, 800) (397, 800)\n",
      "[ 5869   156  2488  1041  7434 26154  1390 12330  6580  5334  2525    58\n",
      "   165   156 26155 25358 12749   650  6207  1132   625  3652 12888  3627\n",
      "   571    58  5469   650  1863 26156  8893 26157  1174  8893  3652   625\n",
      "  1970  3867  3420  1342 13204  4446   650  2901  2056  2971  3556  5411\n",
      "  6036  1491  1492  4414    58  3759   238   285  1833   799  6625  2149\n",
      "  3641  2852   767   696   939    58  1490    79    47  1586  5428    58\n",
      "  1721  2927  1248  4035  4332  6650  6060  1625  7886   896 25929  1735\n",
      "  1003  6405  1249  1010 17756 17757  8749  9327  6537 18935 26151  2980\n",
      "   500   745    79  2244  3570   405  2765   763 12843  1877  7918 13956\n",
      "    58   763 12843 17058 26158 13480 26159 26160   310   763 17914  1491\n",
      "   523  4642  1912  1397   347 13480   500  3004   712  5877  5607 20055\n",
      "   434   694   528  1103  2306 10081  5394 26161  4381    98 21031  1383\n",
      " 25024 26162 26163  2118  1986  4430  1096  1384   474   732 10076 11463\n",
      "   176  7376  6279  2512 18928  2844 16580  4796  1319  1142   282  1444\n",
      "  1469 15652    83  1514  3123 21483  1033  1707 26164  9346  6207  6878\n",
      "    58 26165   513  4999  1060   841 14270  1460   844 21483   683   239\n",
      "  1986  1707 17871  1314 13192  1033  1172   491  1997  3469 13458  2615\n",
      "  6666  1843   726  2310   713  3370 12177    58  3478   374 12177 20928\n",
      "  2970  6502   500  5833   713  6113   327  1422   445   176 26166  4145\n",
      "   150  3239   163  5225  2100  5411  1343   252  5411  1522  2100  9968\n",
      "  1624   252   163  1522  4899  4900 10055  7386  2525 11670 10687  2656\n",
      "  6103   939   593  1968 26167  6580  1221  4411  2854  2949  1448 26168\n",
      " 26169 26170    46 26171  1491   523  3567   960  3142  7369   551   311\n",
      "  1283  6580  5441   448  1105 23881   156  2591   593  1922  2670   428\n",
      "   428   118  9732    91  1491  1492  2764    83  3932   428   288  9508\n",
      "    79  5822  4332  3331  2228  3142   413  2878  1993    58  2878  3142\n",
      "   413  1484  9984 26172   841  4411  1432  1707  2896  1663  1037   170\n",
      "  8743   619  3075  2184  6121    36   337  1740  6644  1829    42   178\n",
      "  3262  6624  4537   337   869   825   540   405  1283  4676    42   625\n",
      "   626  2964  9517   719   287    79   698 11733  3517  4145  4120  6656\n",
      "  1117  2413    45  5203   486  2334   599  3696   169   176  6955  1319\n",
      " 15226   156 11465  5227   445   176  8281 26101 26087  8644  2249  4515\n",
      "  2163    58  2009  1002  3114  1578  6733    29 11754  1389  1153  2967\n",
      "  8481  5098  1622  9525  1000  7429  2043  8769  5349  5635 26173   826\n",
      "  6595  1142  4373    54  6504   252   138  6936  1682 17746    79  1060\n",
      "  3231  5287  3301   123 12739 11539  5286  6525  5287    45  3034    44\n",
      " 12675  8811   152  1909   178   123  2656  1486   153  7953   457   280\n",
      "   362  1318  6769  2157    58   522  1444   125  1098  2008  2362     9\n",
      "  2018  9525 26174    21  1621  2764  2852 14627   118 26175   356  5297\n",
      "  1693    93  9527 12544  2336  6016   692 10807  2756 15594   318 10326\n",
      "   867   123  1342  7481 17489  6466   692   178   527  1751 25167   170\n",
      "  1491  2363   294  1876   913  5069   676  2503   593  1375   598  1491\n",
      "  1378  1336 21119   321 16584   238 26176  5515   676  1051   181   239\n",
      "  1832 11290  1856 26177  1360 15149  4302   317  4249 10218   189 12063\n",
      " 13673   468   453  1908   791  1635    12  3010   560  1718   672    12\n",
      "  1298  1541  2962   192   841  2874  9227    12   466 26178  2350  4662\n",
      "  2735  2600  1673  5816  3900   291    83   585  1105  6634    36   833\n",
      " 26179  7072 26180  4996 26181  4038   832   833 10975  3183   769  3335\n",
      " 13474  6647   471   362   415  1361    71  2485  1464 21490  6549   831\n",
      "  2330  2430  2408   257  1867   192    68  5626 26182 26183  5169  5170\n",
      "  4831  7629   111  4831  1029  1832  3686  1098   896  4494    58  4057\n",
      " 26184  3142   413  1343  5069  4614    59  9358  3203  4419  1465   415\n",
      "  8476  3142   413  9984     6   303 22842 11932  1888  1098  4242   291\n",
      "   126  1416 12918  5482  1065  3361   827  7409 26185 20352   994    16\n",
      "   257   118   177    21  1621   615   665  2938 23967  6501   379 25231\n",
      "  4121   252   513 10762 11083 26186 10135   226  3917  2615 26187  1713\n",
      " 10762  1422  5829  1635    83  1942   318   525  1491  1492   171    41\n",
      "    89  8402   756 26188  2618  6814  1491    91  3728   698  4744 26189\n",
      "    90    91  2109   831  1128   481 26190 26191  1105  1096  2695  1681\n",
      "   140  1703  5813   338   719   237   841   292   489 12121  6314  9979\n",
      "  2712  2056 15394   252  1720     6    66  1319  3108   491  1666  3963\n",
      "  1729  1751  4935    36   337  1740   540   405  4139  5566 23881  1192\n",
      "  2618   574  1482    89   505   960   756  4443   450   272  8644   475\n",
      " 16764  2992   176   257 11463   176  7376   420   445 10110  9811  1238\n",
      "  8281 26101  3223    80  2244    79   336    44   202    37   600   815\n",
      "  3238  1093  5205  1172   174   176  6558 22202   159   745 18548  3007\n",
      "  3406 26192   910   347   625  2244  1101 12111]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# number of most-frequent words to use\n",
    "nb_words = 40000\n",
    "# cut texts after this number of words\n",
    "maxlen = 800\n",
    "\n",
    "# we can change the \"X???_vectorised\" based on the number of cumulative days:\n",
    "X_v_train, X_v_test, y_v_train, y_v_test = train_test_split(\n",
    "    X_5day_vectorised, data_label_5day, test_size=0.20, random_state=32)\n",
    "\n",
    "X_v_pad_train = sequence.pad_sequences(X_v_train,\n",
    "                                       value=word_index[\"<PAD>\"],\n",
    "                                       padding='post',\n",
    "                                       maxlen=maxlen)\n",
    "X_v_pad_test = sequence.pad_sequences(X_v_test, \n",
    "                                      value=word_index[\"<PAD>\"],\n",
    "                                      padding='post',\n",
    "                                      maxlen=maxlen)\n",
    "\n",
    "print(X_v_pad_train.shape, X_v_pad_test.shape)\n",
    "print(X_v_pad_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? We train!\n",
    "\n",
    "### Training using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.535175879396985"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LogisticRegression(random_state=0, solver='lbfgs')\n",
    "\n",
    "v = CountVectorizer()\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', v),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tom/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/tom/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 64 # total LSTM units\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=EMBEDDING_DIM, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(LSTM_DIM, dropout=0.8, recurrent_dropout=0.8))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tom/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1110 samples, validate on 477 samples\n",
      "Epoch 1/7\n",
      "1110/1110 [==============================] - 15s 14ms/step - loss: 0.6950 - acc: 0.5090 - val_loss: 0.6924 - val_acc: 0.5094\n",
      "Epoch 2/7\n",
      "1110/1110 [==============================] - 14s 12ms/step - loss: 0.6916 - acc: 0.5234 - val_loss: 0.6925 - val_acc: 0.5115\n",
      "Epoch 3/7\n",
      "1110/1110 [==============================] - 14s 13ms/step - loss: 0.6903 - acc: 0.5216 - val_loss: 0.6926 - val_acc: 0.5115\n",
      "Epoch 4/7\n",
      " 256/1110 [=====>........................] - ETA: 9s - loss: 0.6874 - acc: 0.5547 "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_v_pad_train, y_v_train, epochs=7, batch_size=batch_size, \n",
    "          shuffle=True, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 53.90%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_v_pad_test, y_v_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to the disk\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#serialize to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "#serialize weights to HDF5\n",
    "model.save_weights(\"model3.h5\")\n",
    "print(\"Saved model to the disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
